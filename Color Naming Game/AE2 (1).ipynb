{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47265272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e936f0",
   "metadata": {},
   "source": [
    "# Task 1: Guessing the name of a color by Bob\n",
    "\n",
    "For this task we will need to:\n",
    "- input several colors in RGB format\n",
    "- clean our data set\n",
    "- remove duplicate RGB values in the data\n",
    "- train a KNN model\n",
    "- use n-fold cross validation to evaluate our model\n",
    "- find which k value results in the most accurate KNN model\n",
    "- produce a performance report with accuracy and visualizations\n",
    "\n",
    "\n",
    "To do the above we will implement a combination of different functions with libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b4628",
   "metadata": {},
   "source": [
    "First is creating a function that allows the user (Alice) to input N sets of RGB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A function is used here to allow for ease of reusibility \n",
    "'''\n",
    "def colour_inputs(n):\n",
    "    '''\n",
    "    Inputs:\n",
    "     - n : the number of rgb sets that will be entered\n",
    "    \n",
    "    Outputs:\n",
    "     - A list of all the rgb values in the format[(R1,G2,B3).. (Rn,Gn,Bn)]\n",
    "     \n",
    "    Uses a for loop from 0 to n which asks the user to respectively input \n",
    "    the R,G,B values. At the end of each loop iteration it will add the values \n",
    "    to the Output list\n",
    "    '''\n",
    "    \n",
    "    RGB_Inputs = [] # initializes an empty list\n",
    "    \n",
    "    for i in range(n):\n",
    "        R = int(input(\"Enter the R value for the color\"))\n",
    "        G = int(input(\"Enter the G value for the color\"))\n",
    "        B = int(input(\"Enter the B value for the color\"))\n",
    "        RGB_Inputs.append([R,G,B]) #adds the values to the list\n",
    "    \n",
    "    return RGB_Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491b41e",
   "metadata": {},
   "source": [
    "After this function we must import our dataset and prep it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the pd.read_csv method to read in the file, then skips the first 3 lines\n",
    "which are comments detailing the file, finally set the 4th line as the column\n",
    "names as it details what each column means\n",
    "'''\n",
    "colors = pd.read_csv('colour_naming_data-1.csv', skiprows = 3, \n",
    "                    header =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quickly inspect the file to make sure it is read in correctly\n",
    "colors.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe810ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7e9ce",
   "metadata": {},
   "source": [
    "From this quick inspection, I can see that punctuation needs to be removed and that the capitalization also needs to be standardized. So I will proceed by creating a clean function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data, labels):\n",
    "    '''\n",
    "    Input: Dataframe name and a list of all column names\n",
    "    Output: cleaned dataframe\n",
    "    Works by looping through each column, if the column is of type string\n",
    "    then it will lower all the words and remove any excess space. For all\n",
    "    columns it will remove all punctuation through the use of regex operators\n",
    "    '''\n",
    "    #Loops through the columns\n",
    "    for i in labels:\n",
    "        #if statement to check if the column is of type string\n",
    "        if data[i].dtype == 'object':\n",
    "            #uses a lambda function to lower and strip all the values\n",
    "            data[i] = data[i].apply(lambda x: x.lower().strip())\n",
    "            \n",
    "        #Line of code that removes punctuation  \n",
    "        data[i] = data[i].replace('[^\\w\\s]', ' ', regex=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_colors = clean(colors, ['sample_id', 'colour_name', 'R', 'G', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quickly reinspect the data with the new cleaned df\n",
    "cleaned_colors.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_colors.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62dbcb",
   "metadata": {},
   "source": [
    "Since our code is now cleaned we must now remove all duplicate RGB values, and set the remaining values to the most common color name associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To do so we will create a new data frame that is grouped by RGB values,\n",
    "then we will use the value_counts and idxmax functions to find what color\n",
    "is most common for that rgb value. Finally by using reset_index we put the \n",
    "indices back to default\n",
    "\n",
    "These functions were found by looking through the pandas documentation \n",
    "and seeing how they affect a dataframe. \n",
    "'''\n",
    "#first uses the groupby function to arrange the original dataframe by RGB\n",
    "colors_df = cleaned_colors.groupby(['R','G','B'])\n",
    "\n",
    "#Then uses a lambda function to find what color name is most used for that rgb value\n",
    "colors_df = colors_df['colour_name'].apply(lambda x: \n",
    "                                        x.value_counts().idxmax())\n",
    "\n",
    "#finally resets the indices \n",
    "colors_df = colors_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb3d6b",
   "metadata": {},
   "source": [
    "Now our data is fully ready to train a model. I also notice that some colors repeat several times which leads me to predict that when we train our model, it will be slighlty bias and overselect these colors that repeat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6455eec",
   "metadata": {},
   "source": [
    "We will create a function to create a knn model, evaluate using n-fold cross validation and then finally return some performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca8b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(n, x, y):\n",
    "    '''\n",
    "    Input: \n",
    "     - n: the number of neighbors for the kNN model\n",
    "     - x: the x-value from our dataset (rgb_representations)\n",
    "     - y: the y-value from our dataset (color_names)\n",
    "     \n",
    "     Output: \n",
    "     - predictions\n",
    "     - accuracy score\n",
    "     - weighted precision \n",
    "     - weighted recall\n",
    "     - weighted f1\n",
    "     \n",
    "     This function works by first creating the kNN model of k(n). Then by using the LeaveOneOut classifier,\n",
    "     we train our knn model on every single data point. Then we use the X_Test set created by the classifier \n",
    "     to predict what color_name is associated with those rgb values. Finally we use the predictions generated\n",
    "     by the knn and the sklearn metrics library to generate values for accuracy, precision, recall, and f1. \n",
    "    '''\n",
    "   \n",
    "    knn = KNeighborsClassifier(n_neighbors = n) # initializes the kNN model with k=n\n",
    "    loo = LeaveOneOut() # initializes the leaveOneOut classifier\n",
    "    \n",
    "    #creates empty variable to later store the values generated from the classification/training\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    ac=0\n",
    "    wp=0\n",
    "    wr=0\n",
    "    wf=0\n",
    "    \n",
    "    #for loop to go through every-single data point in x\n",
    "    for train_index, test_index in loo.split(x):\n",
    "        #Assigns a training set and test set\n",
    "        X_Train, X_Test = x[train_index], x[test_index]\n",
    "        Y_Train, Y_Test = y[train_index], y[test_index]\n",
    "        \n",
    "        #uses the training set to train the knn model\n",
    "        knn.fit(X_Train, Y_Train)\n",
    "        \n",
    "        #then uses the x_test variable to generate predictions on a value for Y\n",
    "        predicted = knn.predict(X_Test)\n",
    "        \n",
    "        #adds the prediction values to 'predictions' and the actual values (y-values) to 'actual_labels'\n",
    "        predictions.extend(predicted)\n",
    "        actual_labels.extend(Y_Test)\n",
    "    \n",
    "   \n",
    "    #uses the sklearn.metrics library to generate performance metrics\n",
    "    ac = metrics.accuracy_score(actual_labels, predictions)\n",
    "    '''\n",
    "    the average is weighted to account for the different classifications of colors\n",
    "    the zero_division =0.0 helps to avoid the issues that arise because there are 0s in the set by setting \n",
    "    anytime that a 0/0 appears to just equal 0\n",
    "    '''\n",
    "    wp = metrics.precision_score(actual_labels,predictions,\n",
    "                                 average = 'weighted', zero_division = 0.0)\n",
    "    \n",
    "    wr = metrics.recall_score(actual_labels, predictions, \n",
    "                              average = 'weighted', zero_division = 0.0)\n",
    "    \n",
    "    wf = metrics.f1_score(actual_labels, predictions, \n",
    "                          average = 'weighted', zero_division = 0.0)\n",
    "    \n",
    "    return predictions,ac, wp, wr, wf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ed0e7",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning and generating a performance report\n",
    "\n",
    "We are going to test our kNN model for all k-values from 1-12 and compare all the metrics that are generated as a result to find which model is most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates empty lists to store all the metrics/values that are generated\n",
    "predictions = []\n",
    "accuracy = []\n",
    "weighted_precision = []\n",
    "weighted_recall = []\n",
    "weighted_f1 = []\n",
    "\n",
    "#converts the RGB values and the color names to numpy arrays so they can be ran through the function\n",
    "x = colors_df[['R', 'G', 'B']].to_numpy()\n",
    "y = colors_df['colour_name'].to_numpy()\n",
    "\n",
    "#Runs a for loop from 1-12 \n",
    "for i in range(1,13):\n",
    "    #runs the training function\n",
    "    p, ac, wp, wr, wf = training(i,x,y) \n",
    "    #adds all the generated values to the lists\n",
    "    predictions.append(p)\n",
    "    accuracy.append(ac)\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "    weighted_f1.append(wf)\n",
    "    \n",
    "    #Generates a small report on each value\n",
    "    print(f\"\\nResults for K: {i}\\nAccuracy: {ac}\\nWeighted Precision: {wp}\")\n",
    "    print(f\"Weighted Recall: {wr}\\nWeighted F1: {wf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27baff47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91da225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds for k-value was the accuracy the greatest \n",
    "greatest_accuracy_index = accuracy.index(max(accuracy))\n",
    "print(f\"The k-value with the greatest accuracy was {greatest_accuracy_index+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1c5a2",
   "metadata": {},
   "source": [
    "Now we will use the plotly library to create plots that show how each metric varies dependant on the k-value. I'm using plotly instead of matplotlib for its ease of readability and interactive functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aea3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes a 2x2 set of subplots for each metric\n",
    "fig = make_subplots(rows = 2, cols = 2,\n",
    "                    subplot_titles = (\"Accuracy\", \"Weighted Precision\", \"Weighted Recall\", \"Weighted F1\"),\n",
    "                   horizontal_spacing = 0.4)\n",
    "\n",
    "#creates a list of numbers from 1-12 to serve as our x-axis\n",
    "X = [*range(1,13)]\n",
    "\n",
    "#plots all the points\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X, y = accuracy, name = \"Accuracy Scores\"),\n",
    "    row = 1, col =1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X, y = weighted_precision, name = \"Weighted Precision Scores\"),\n",
    "    row = 1, col = 2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X, y = weighted_recall, name = \"Weighted Recall Scores\"),\n",
    "    row = 2, col = 1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X, y = weighted_f1, name = \"Weighted F1 Scores\"),\n",
    "    row = 2, col = 2\n",
    ")\n",
    "\n",
    "#Adds title and adjusts size\n",
    "fig.update_layout(height = 600, width = 600, title_text = \"Performance Report\")\n",
    "\n",
    "#Labels x-axes\n",
    "fig.update_xaxes(title_text = \"K_Values\", row = 1, col = 1)\n",
    "fig.update_xaxes(title_text = \"K_Values\", row = 1, col = 2)\n",
    "fig.update_xaxes(title_text = \"K_Values\", row = 2, col = 1)\n",
    "fig.update_xaxes(title_text = \"K_Values\", row = 2, col = 2)\n",
    "\n",
    "#Labels y-axes\n",
    "fig.update_yaxes(title_text = \"Accuracy of kNN\", row = 1, col = 1)\n",
    "fig.update_yaxes(title_text = \"Precision of kNN\", row = 1, col = 2)\n",
    "fig.update_yaxes(title_text = \"Recall of kNN\", row = 2, col = 1)\n",
    "fig.update_yaxes(title_text = \"F1 of kNN\", row = 2, col = 2)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013fcb5",
   "metadata": {},
   "source": [
    "## What I notice\n",
    "\n",
    "All the graphs seem to increase from a higher k-value, except for the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds a column to the color_df to have all the predictions from the highest accuracy model side-by-side\n",
    "colors_df['predictions'] = predictions[greatest_accuracy_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ebf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uses the plotly library to create a 3D scatter plot where colors are plotted based on their RGB-value \n",
    "Then they are labeled based on what the kNN model of the greatest accuracy predicted their values to be. \n",
    "'''\n",
    "fig = px.scatter_3d(colors_df, x = 'R', y = 'G', z = 'B',\n",
    "                   color = 'colour_name', text = 'predictions')\n",
    "\n",
    "fig.update_traces(textposition = 'top center')\n",
    "\n",
    "fig.update_layout(\n",
    "    height = 600,\n",
    "    width = 800,\n",
    "    title_text = \"Predicted Colors and their Actual Actual Color\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe4ffb",
   "metadata": {},
   "source": [
    "## What I notice\n",
    "\n",
    "There's alot of repeats in predictions which is most likely due to how many colors repeated in the dataframe. But also it was these predictions that were the most accurate since it appears so much. The ones that were least accurate had to be the points that were synonyms of a color (ie: moave, violet). The program had often just assigned the general name to these colors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5af331",
   "metadata": {},
   "source": [
    "# Task 2: Assessing color names\n",
    "\n",
    "For task 2 we will need to:\n",
    " - Make a set of N=10 colors in RGB format\n",
    " - measure the distance between the test colors and predicted names\n",
    " - create a weighted graph of color synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c48563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses the previously created colour_inputs function to create a list of 10 RGB values\n",
    "alice = colour_inputs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_centroid(data, value_column , value):\n",
    "    '''\n",
    "    Input:\n",
    "     - Data: Dataframe \n",
    "     - value_column: the column to search in\n",
    "     - value: the color to search for\n",
    "    \n",
    "    Output:\n",
    "     - A centroid for the inputted value \n",
    "    \n",
    "    This function works by first located all the rgb_values that pertain\n",
    "    to the inputted color_name. Then it averages out all those rgb value\n",
    "    to determine a centroid for that color\n",
    "    '''\n",
    "    \n",
    "    #Creates empty variables to store the centroid  RGB values\n",
    "    R,G,B = 0,0,0\n",
    "    \n",
    "    #sorts and stores the dataframe to only have the value we are looking for\n",
    "    rgb_values = data.loc[data[value_column] == value]\n",
    "    #stores the length of our small dataframe\n",
    "    length = len(rgb_values)\n",
    "    \n",
    "    #a for-loop that goes through our small dataframe and adds together \n",
    "    #the R,G,B values respectively\n",
    "    for index, row in rgb_values.iterrows():\n",
    "        R += row['R']\n",
    "        G += row['G']\n",
    "        B += row['B']\n",
    "    \n",
    "    #averages each RGB Value and then returns it in a list\n",
    "    centr = [R/length, G/length, B/length]\n",
    "    return np.array(centr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72602f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_color(data,value_column,R,G,B):\n",
    "    '''\n",
    "    Input: \n",
    "     - data: Dataframe\n",
    "     - value_column: the column to search in\n",
    "     - R,G,B: the RGB Values\n",
    "    Output:\n",
    "     - The color associated with that RGB value\n",
    "     \n",
    "     This function is created to allow for an easier way to search \n",
    "     our dataframe for a color_label based on inputted rgb values. It\n",
    "     works by first sorting the dataframe to only have the value with the\n",
    "     exact same RGB value, then converts the df column into a list and \n",
    "     returns the color value\n",
    "    '''\n",
    "    #Sorts and stores the dataframe based on RGB\n",
    "    a = data.loc[(data['R'] == R) & \n",
    "                 (data['G'] == G) & \n",
    "                 (data['B'] == B)]\n",
    "    #converts and stores the sorted dataframe column that we're looking for\n",
    "    b = list(a[value_column])\n",
    "    \n",
    "    #returns the color value\n",
    "    return b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f72c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(a,b):\n",
    "    '''\n",
    "    Input: \n",
    "     - a: first vector\n",
    "     - b: second vector\n",
    "    Output:\n",
    "     - euclidean distance between a and b\n",
    "    Quick function to make it easier to calculate euclidean distance\n",
    "    '''\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a438ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we need to calculate the distance between Alice's actual values\n",
    "and Bob's predicted values\n",
    "'''\n",
    "\n",
    "#Empty list to later store the distance values\n",
    "distance = []\n",
    "\n",
    "#A for loop to iterate through the list of alice's inputs\n",
    "for i in alice:\n",
    "    #First finds the predictions and actual correlated to the \n",
    "    #rgb values that alice inputted\n",
    "    predicted_color = find_color(colors_df, \n",
    "                                 'predictions', \n",
    "                                 i[0], i[1], i[2])\n",
    "    \n",
    "    alice_color = find_color(colors_df,\n",
    "                             'colour_name', \n",
    "                             i[0], i[1], i[2])\n",
    "    \n",
    "    #Then calculates the centroid of that predicted color\n",
    "    predicted_centroid = rgb_centroid(colors_df, \n",
    "                                      'predictions', \n",
    "                                      predicted_color)\n",
    "    \n",
    "    #Finally finds the euclidean distance between the array of\n",
    "    #alice's values and array of the centroid, and also appends it to the distance list\n",
    "    dist = euclidean(np.array(i),predicted_centroid)\n",
    "    distance.append(dist)\n",
    "    \n",
    "    #A small print statement to show what color Alice put, what Bob predicted\n",
    "    #and the distance between them\n",
    "    print(f\"\\nThe distance between Alice's {alice_color} and Bob's {predicted_color} is {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c01062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataframe to store the euclidean distances and trial number\n",
    "results_df = pd.DataFrame(distance, columns = ['Euclidean Distance'])\n",
    "results_df['Color #'] = range(1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df15c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the dataframe with x being the trial number and y being \n",
    "#the euclidean distance\n",
    "fig = px.bar(results_df, x= 'Color #', y= 'Euclidean Distance', \n",
    "             title= \"Distance between Bob's Predicted Colors and Alice's Actual Colors\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f260443",
   "metadata": {},
   "source": [
    "## What I notice\n",
    "\n",
    "The graph looks for the most part very symmetric which most likely is due to the order that I inputted the rgb values. I predicted that the times where the program correctly guessed the color would have a distance of 0, however (like in trial #10) they often had a large distance. This is because of the fact we are taking the centroid and the same color labels appear many times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd839f7a",
   "metadata": {},
   "source": [
    "# Creating a weighted graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import collections as col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54822404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    '''\n",
    "    This class is to allow for an easier use of the networkx library and \n",
    "    its functionalities of adding vertices, edges, and drawing graphs.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, V = [], E = []):\n",
    "        \n",
    "        '''\n",
    "        this init function reads a list of vertices and a list of edges, \n",
    "        it then proceeds to add the vertices and edges to the graph.\n",
    "        '''\n",
    "        \n",
    "        #declares a networkx graph\n",
    "        self.G = nx.Graph()\n",
    "        \n",
    "        #iterates through the vertex list and adds them to the graph\n",
    "        for v in V:\n",
    "            self.add_vertex(v)\n",
    "        \n",
    "        #iterates through the edge list and adds an edge and weight between\n",
    "        #the two vertices\n",
    "        for u, v, w in E:\n",
    "            self.add_edge(u, v, w)\n",
    "         \n",
    "    def add_vertex(self, v):\n",
    "        '''\n",
    "        This add_vertex function takes in an input of a vertex then \n",
    "        checks if the vertex is in the graph. If not, it proceeds to \n",
    "        add a new node of v into the graph. \n",
    "        '''\n",
    "        if v not in self.G:\n",
    "            self.G.add_node(v)\n",
    "        \n",
    "    def add_edge(self, u, v, w):\n",
    "        '''\n",
    "        This add_edge function takes in 3 values. u represents the first node,\n",
    "        v represents the second node, and w represents the weight \n",
    "        between the nodes. It works by adding the two vertices if they're not\n",
    "        already in the graph, then checks if u and v are equal. If they aren't\n",
    "        then it adds an edge between the two vertices with weight w. \n",
    "        The reason why this check is important is so we don't get a vertex\n",
    "        going to itself.\n",
    "        '''\n",
    "        self.add_vertex(u)\n",
    "        self.add_vertex(v)\n",
    "        \n",
    "        if u != v:\n",
    "            self.G.add_edge(u,v, weight = w)\n",
    "\n",
    "        \n",
    "    def visualize(self):\n",
    "        '''\n",
    "        This visualize function utilizes the nx.draw() function to create\n",
    "        a graph of certain specifications. It works by creating a plot of\n",
    "        20x10 dimensions, then finding the values of the edges and weights,\n",
    "        then creating a spring_layout to space out the nodes, finally plots\n",
    "        the nodes and edges with edge_color changing depending on weight.\n",
    "        '''\n",
    "        plt.figure(figsize = (20,10))\n",
    "        \n",
    "        \n",
    "        edges,weights = zip(*nx.get_edge_attributes(self.G,\n",
    "                                                   'weight').items())\n",
    "        \n",
    "        pos = nx.spring_layout(self.G, scale = 1.5)\n",
    "        nx.draw(self.G, pos, edgelist = edges, \n",
    "                node_color = \"#ffffff\",\n",
    "                edge_color = weights, with_labels = True, \n",
    "                node_size = 1500, font_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d495be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of all unique color names\n",
    "vertices = list(set(colors_df['colour_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of all the edges and weights \n",
    "edges = [] \n",
    "#loops through the list of vertices\n",
    "for i in vertices:\n",
    "    #finds the centroid for color1\n",
    "    centr1 = rgb_centroid(colors_df, 'colour_name', i)\n",
    "    #loops through list again to get the second vertex\n",
    "    for j in vertices:\n",
    "        #finds the centroid for color2\n",
    "        centr2 = rgb_centroid(colors_df, 'colour_name', j)\n",
    "        \n",
    "        #Adds to the list of all the edges and weights\n",
    "        edges.append((i,j,euclidean(centr1,centr2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a Graph G\n",
    "G = Graph(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294faf84",
   "metadata": {},
   "source": [
    "# Reflection on AE2\n",
    "\n",
    "AE2: The Colour Language Game had proven to be a challenging yet rewarding task, one that forced me to create new stems of knowledge and grow as a future data professional. The first challenge came in the form of the instructions. As I'm still very new and budding to this form of machine learning, the instructions at first were unfamiliar and many times I was left puzzled as to how I should even approach a certain task. However, I combatted this by reading the instructions line-by-line and writing out my thoughts on a piece of pencil and paper. I detailed what I thought each task meant and how to implement the functionalities necessary to complete the task above. For example, by removing the excess RGB values and finding the most common color_label, I researched the panda's library and found the value_counts() and idxmax() functions, which completely solved this task. This process of writing and researching before typing out code was beneficial as it allowed me to stay organized. Another challenge I found was in the process of creating graphs. When I originally wrote this program, I used matplotlib for all the scatterplots, bar graphs, etc. However, when using this library I found that the graphs had often remained cluttered and the process of organizing/labeling each point would prove to only further clutter my graphs. Determined to make my project appear as neat as possible, I found the plotly library which is now one of my favorite implementations to use in Python. This library allows me to create easy interactive graphs that would otherwise be impossible for someone of my lacking skill to create. This process also taught me the importance of reading documentation for libraries as I was able to learn how to implement subplots. \n",
    "\n",
    "Beyond the challenges, I believe my techniques in general had been very effective in completing this task at hand. For example, in training and generating a performance report, I created a function to not only train the model but also calculate all the necessary metrics. However, one area that can be improved is the actual accuracy of the model as it hovers around a very low 40%. I believe this comes from the dataset having colors that often repeat. In the future, I hope to fix this accuracy by either adding more data points or by further sorting my data to make it so each color name has exactly one RGB representation. Furthermore, I hope to improve my code by using more classes and object-oriented design principles to allow for a more streamlined process of analyzing the data. For example, in the graphs section, I used a class to make it easier to actually create the graph. In general, I am proud of myself for completing this project and learning several new techniques that will push me to new heights as I continue on my Data Science journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf39af",
   "metadata": {},
   "source": [
    "# Extra Functionality\n",
    "\n",
    "For extra functionality I wanted to add a pie-chart that shows how much each color appears in the list of predictions and the list of actual color labels. This is because the data had been very skewed by these representations, and I desired to find which colors most affected the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To create the pie charts I will use the plotly library and create 2 subplots of side-by-side charts. One chart will\n",
    "represent the color distribution in the actual color labels with the other being in the predictions. This program \n",
    "works by first setting up the subplot of (1x2), then creating 2 pie charts, making them neater, and labeling the \n",
    "title\n",
    "'''\n",
    "#Creates the subplots\n",
    "fig = make_subplots(rows = 1, cols =2, specs = [[{'type' : 'domain'},\n",
    "                                                {'type':'domain'}]])\n",
    "\n",
    "fig.add_trace(go.Pie(labels = colors_df['colour_name'], name = 'colour_name', title = \"Actual Color Labels\"), \n",
    "             1, 1)\n",
    "\n",
    "fig.add_trace(go.Pie(labels = colors_df['predictions'], name = 'predictions', title =\"Predictions\"), \n",
    "             1, 2)\n",
    "\n",
    "#Removes trace lines \n",
    "fig.update_traces(textposition='inside')\n",
    "fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\n",
    "\n",
    "#titles the graph\n",
    "fig.update(layout_title_text = 'Color Distributions between Actual Labels and Predictions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a2fc04",
   "metadata": {},
   "source": [
    "## What I noticed\n",
    "\n",
    "After this analysis I realize the predictions almost directly mirror how the actual color labels are. For example, in both pie charts the \"Pink\" and \"purple\" color labels had appeared the most, while more niche colors appeared less. I strongly believe this unevenness of data is what leads to my kNN model to be less accurate. To fix this I would add more data points to the smaller categories to hopefully even out the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3620bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
